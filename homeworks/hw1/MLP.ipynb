{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wx0DEHvrwYji"
   },
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "\n",
    "Neural Network / Fall 1399, Iran University of Science and Technology\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJQpO_5YOgf5"
   },
   "source": [
    "## 1. MLP from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ea3v_7iWOr-N"
   },
   "source": [
    "In this assignment, you will explore and implement the properties of a primary deep learning model called ***multilayer perceptron(MLP)***. Basically, the goal of an MLP is to learn a non-linear mapping from inputs to outputs. We can show this mapping as $y = f(x; \\theta)$ , where $x$ is the input and $\\theta$ is a vector of all the parameters in the network, which we're trying to learn.  \n",
    "\n",
    "\n",
    "As you see in the figure, every MLP network consists of an input layer, an output layer, and one or more hidden layers in between. Each layer consists of one or more cells called Neurons. In every Neuron, a dot product between the inputs of the cell and a weight vector is calculated. The result of the dot product then goes through a non-linear function (activation function e.g. $tanh$ or $sigmoid$) and gives us the output of the neuron.\n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=https://static.javatpoint.com/tutorial/tensorflow/images/multi-layer-perceptron-in-tensorflow.png width=\"500\" align=\"center\">\n",
    "</center>\n",
    "\n",
    "\n",
    "<br>\n",
    "Thoughout this assignment, inputs will be matrices with the shape of $b \\times M$ where $b$ is the batch size and $M$ is the number of features of inputs. <br>\n",
    "As for the equations, let's compute the output of the $i$th layer:\n",
    "$$A^i = f(A^{i-1}w^i + b^i)$$\n",
    "\n",
    "Imagine that $(i-1)$th and $i$th layer have sizes of $n$ and $p$ respectively. The dimensions of weight and bias will be as follows:\n",
    "<br><br>\n",
    "$$w^{n\\times p} , b^{1\\times p}$$\n",
    " <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "87vFrWhN3bub"
   },
   "source": [
    "Numpy is the only package you're allowed to use for implementing your MLP in this assignment, so let's import it in the cell below! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gvunZcHgOe_"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uvoXV-1lLXu-"
   },
   "source": [
    "### 1.1 Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpfexSvg3yoK"
   },
   "source": [
    "Now let's implement some activation functions! Linear, Relu and Sigmoid are the functions that we'll need in this assignment. Note that you should also implement their derivatives since you'll need them later for back-propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2tfbJgD2GCdm"
   },
   "outputs": [],
   "source": [
    "## We've implemented the Linear activation function for you\n",
    "\n",
    "def linear(x, deriv=False):\n",
    "    return x if not deriv else np.ones_like(x)\n",
    "\n",
    "def relu(x, deriv=False):\n",
    "    ''' \n",
    "    Args:\n",
    "        x: A numpy array of any shape \n",
    "    deriv: True or False. determines if we want the derivative of the function or not.\n",
    "\n",
    "    Returns:\n",
    "        relu_out: A numpy array of the same shape as x. \n",
    "    Basically relu function or its derivative applied to every element of x\n",
    "\n",
    "    '''\n",
    "    relu_out = x\n",
    "    if(deriv):\n",
    "        relu_out = 1*(x >= 0)\n",
    "    else:\n",
    "        relu_out = np.maximum(x, 0)\n",
    "\n",
    "    return relu_out\n",
    "  \n",
    "def  sigmoid(x, deriv=False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: A numpy array of any shape \n",
    "        deriv: True or False. determines if we want the derivative of the function or not.\n",
    "\n",
    "    Returns:\n",
    "        sig_out: A numpy array of the same shape as x. \n",
    "    Basically sigmoid function or its derivative applied to every element of x\n",
    "\n",
    "    \"\"\"\n",
    "    sig_out = 1/(1 + np.exp(-x))\n",
    "    if(deriv):\n",
    "            sig_out = sig_out * (1 - sig_out)\n",
    "    return sig_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70XLtOxeKEV0"
   },
   "source": [
    "**Question**: Why do activation functions have to be non-linear? Could any non-linear function be used as an activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6WI7RhEkK_fK"
   },
   "source": [
    "<font color=red>**Answer**: The purpose of the activation function is to introduce non-linearity into the network. In turn, this allows us to model a response variable (aka target variable, class label, or score) that varies non-linearly with its explanatory variables.\n",
    "\n",
    "non-linear means that the output cannot be reproduced from a linear combination of the inputs (which is not the same as output that renders to a straight line--the word for this is affine).\n",
    "\n",
    "Another way to think of it: without a non-linear activation function in the network, a NN, no matter how many layers it had, would behave just like a single-layer perceptron, because summing these layers would give you just another linear function.\n",
    "\n",
    "But here, in MLP models, the classic idea of the learning algorithm is to use gradian descent. For this reason, the activation function has to be continuous functions of the weights and differentiable everywhere. Thus, we can't use each arbitrary non-linear function we want for this objective.</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T2DMSRxfLwkz"
   },
   "source": [
    "### 1.2 Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRyGqmxXMkh-"
   },
   "source": [
    "Now let's implement our MLP class. This class handles adding layers and doing the forward propagation. Here are the attributes of this class:\n",
    "<br> -  __parameters__: A list of dictionaries in the form of _{'w': weight, 'b': bias}_ where _weight_ and _bias_ are weight matrix and bias vector of a layer.\n",
    "<br>- __act_funcs__: A list of activation functions used in the corresponding layer.\n",
    "<br>- __activations__: A list of matrices each corresponding to the output of each layer.\n",
    "<br>- __weighted_ins__: A list of matrices each corresponding to the weighted input of each layer. Weighted input, as the name \n",
    "suggests, is layer's input multiplied by layer's weights and added to layer's bias. Which then goes into the layer's activation function to compute the layer's activations(outputs)!\n",
    "<br> Note that we store weighted inputs and outputs of the layers because we'll need them later for implementing the back-propagation algorithm. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0LSOmEAgONii"
   },
   "source": [
    "You only need to complete the _feed_forward_ function in the MLP class. This function performs forward propagation on the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvgYrWCNj11P"
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: An integer determining the inpu dimension of the MLP\n",
    "                \n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.parameters = []\n",
    "        self.act_funcs = []\n",
    "        self.activations = []\n",
    "        self.weighted_ins = []\n",
    "\n",
    "    def add_layer(self, layer_size, act_func=linear):\n",
    "        \"\"\"\n",
    "        Add layers to the MLP using this function\n",
    "        Args:\n",
    "            layer_size: An integer determinig the number of neurons in the layer\n",
    "            act_func: A function applied to the units in the layer        \n",
    "        \"\"\"\n",
    "        ### Size of the previous layer of mlp\n",
    "        prev_size = self.input_dim if not self.parameters else self.parameters[-1]['w'].shape[-1]\n",
    "\n",
    "        ### Weight scale used in He initialization\n",
    "        weight_scale = np.sqrt(2/prev_size)\n",
    "        ### initializing the weights and bias of the layer\n",
    "        weight = np.random.normal(size=(prev_size, layer_size))*weight_scale\n",
    "        bias = np.ones(layer_size) *0.1\n",
    "        ### Add weights and bias of the layer to the parameters of the MLP\n",
    "        self.parameters.append({'w': weight, 'b': bias})\n",
    "        ### Add the layer's activation function \n",
    "        self.act_funcs.append(act_func)\n",
    "\n",
    "\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Propagate the inputs forward using this function\n",
    "        Args:\n",
    "            X: A numpy array of shape (b, input_dim) where b is the batch size and input_dim is the dimension of the input\n",
    "        Returns:\n",
    "            mlp_out: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the dimension of the output\n",
    "        Hint: Don't forget to store weighted inputs and outputs of each layer in self.weighted_ins and self.activations respectively            \n",
    "        \"\"\"\n",
    "        self.activations = []\n",
    "        self.weighted_ins = []\n",
    "        mlp_out = X\n",
    "        counter = 0\n",
    "        for layer_params in self.parameters:\n",
    "            mlp_out = np.dot(mlp_out, layer_params['w']) + layer_params['b']\n",
    "            self.weighted_ins.append(mlp_out)\n",
    "            mlp_out = self.act_funcs[counter](mlp_out, False)\n",
    "            self.activations.append(mlp_out)\n",
    "            counter += 1\n",
    "\n",
    "        return mlp_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kx2D_UjSs7Ly"
   },
   "source": [
    "__Question__: In the _add_layer_ function of the MLP class, we used a method called _He initialization_ to initialize the weights. Explain how this method can help with the training of an MLP?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJX-xv3UxmCn"
   },
   "source": [
    "<font color=red>The aim of weight initialization is to prevent layer activation outputs from exploding or vanishing during the course of a forward pass through a deep neural network. If either occurs, loss gradients will either be too large or too small to flow backwards beneficially, and the network will take longer to converge, if it is even able to do so at all.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKSi_6Qkxo4D"
   },
   "source": [
    "### 1.3 Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JjqaOSDMxuRc"
   },
   "source": [
    "In the previous sections, we implemented an MLP that accepts an input $x$ and propagates it forward and produces an output $\\hat{y}$. The next step in implementing our MLP is to see how good our network's output $\\hat{y}$ is compared to the target output $y$! This is where the loss function comes in. This function gets $y$ and $\\hat{y}$ as its inputs and returns a scaler as its output. This scaler indicates how good current parameters of the network are. <br>\n",
    "the choice of this function depends on the task, e.g regression or binary classification. Since you'll be doing a multiclass classification later in this assignment, let's implement the cross-entropy function. Cross-entropy is the function mostly used for classification tasks but to use it in a multiclass setting, the network's outputs must be passed through a softmax activation function and the target output must be in one-hot encoded format.<br>\n",
    "<center>\n",
    "<img src=https://miro.medium.com/max/1838/1*lqHLeRpETQL0Z5lk1euaJA.png width=\"500\" align=\"center\">\n",
    "</center>\n",
    "<br>\n",
    "$$Softmax(\\hat{y})_i =  \\frac{e^{\\hat{y}_i}}{\\sum^{C}_j e^{\\hat{y}_j}} $$ <br>\n",
    "$$ Cross Entropy(y, \\hat{y}) = -\\sum_i^C {y_i log(Softmax(\\hat{y})_i)}$$\n",
    "Where $y$ and $\\hat{y}$ are two one-hot encoded vectors. $y$ is a single target label and $\\hat{y}$ is a single output.<br>\n",
    "Now let's first implement the softmax activation function! Note that the above formulas are for a single sample, however you should implement the batch version!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6WGCohQUivOS"
   },
   "outputs": [],
   "source": [
    "def softmax(y_hat):\n",
    "    \"\"\"\n",
    "    Apply softmax to the inputs\n",
    "    Args:\n",
    "    y_hat: A numpy array of shape (b, out_dim) where b is the batch size and out_dim is the output dimension of the network(number of classes) \n",
    "\n",
    "    Returns:\n",
    "    soft_out: A numpy array of shape (b, out_dim)\n",
    "\n",
    "    \"\"\"\n",
    "    soft_out = np.exp(y_hat)\n",
    "    soft_out = soft_out/soft_out.sum(axis=1, keepdims=True)\n",
    "    return soft_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRsstrQQHXYu"
   },
   "source": [
    "Now implement the categorical cross-entropy function (\"categorical\" refers to multiclass classification). Note that the inputs are in batches, so the loss of a batch of samples will be the average of losses of samples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dt9YcJAr8ADF"
   },
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y, y_soft):\n",
    "    \"\"\"\n",
    "    Compute the categorical cross entropy loss\n",
    "    Args:\n",
    "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
    "    y_soft: A numpy array of shape (b, out_dim). Output of the softmax activation function\n",
    "\n",
    "    Returns:\n",
    "    loss: A scaler of type float. Average loss over a batch.\n",
    "\n",
    "    Hint: Use np.mean to compute average loss of a batch\n",
    "\n",
    "    \"\"\"\n",
    "    loss = []\n",
    "    for label, pred in zip(y, y_soft):\n",
    "        pred /= pred.sum(axis=-1, keepdims=True)\n",
    "        loss.append(np.sum(label * -np.log(pred), axis=-1, keepdims=False))\n",
    "    loss = np.mean(loss)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQyCUJ0jSRUf"
   },
   "source": [
    "Great! You have implemented both softmax and categorical cross-entropy functions. Now instead of applying softmax activation function to the output layer of the MLP and then using categorical cross-entropy as loss function, we can merge these two steps and make a softmax categorical cross-entropy loss function and use linear activation function in the output layer! The reason behind this is that the gradient of the softmax categorical cross-entropy loss with respect to the MLP's output is efficiently calculated as:\n",
    "<br>\n",
    "\n",
    "$$ Softmax(\\hat{y}) - y$$\n",
    "\n",
    "for a single sample. Here $\\hat{y}$ is the MLP's output and $y$ is the target output (labels).<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkZZovW27k3T"
   },
   "source": [
    "Now let's implement the softmax categorical cross-entropy function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VzbOtjnJMohT"
   },
   "outputs": [],
   "source": [
    "def softmax_categorical_cross_entropy(y, y_hat, return_grad=False):\n",
    "    \"\"\"\n",
    "    Compute the softmax categorical cross entropy loss\n",
    "    Args:\n",
    "    y: A numpy array of shape (b, out_dim). Target labels of network.\n",
    "    y_hat: A numpy array of shape (b, out_dim). Output of the output layer of the network\n",
    "    return_grad: If True return gradient of the loss with respect to y_hat. If False just return the loss\n",
    "\n",
    "    Returns:\n",
    "    loss: A scaler of type float. Average loss over a batch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    y_soft = softmax(y_hat)\n",
    "\n",
    "    if not return_grad:\n",
    "        loss = categorical_cross_entropy(y, y_soft)\n",
    "        return loss\n",
    "    else:\n",
    "        loss_grad = (y_soft - y)/y.shape[0]\n",
    "        return loss_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CenxHCNZD-5V"
   },
   "source": [
    "### 1.4 Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66l6gCb9EDvW"
   },
   "source": [
    "After calculating the loss of the MLP, we need to propagate this loss back to the hidden layers in order to calculate the gradient of the loss with respect to the weights and biases of the network. The algorithm used to calculate these gradients is called back-propagation or simply backprop. Backprop uses chain rule to compute the gradients of the network parameters. Now let's go over the steps of this algorithm (This is the fully matrix-based version):\n",
    "- calculate gradient of the loss with respect to $\\hat{y}$\n",
    "<br> $g \\longleftarrow \\nabla_\\hat{y} Loss$ \n",
    "- for each layer $L$ starting from the ouput layer: <br>\n",
    "&emsp;&emsp; $g \\longleftarrow g \\odot f^\\prime(weightedInput^{(L)})$ &emsp; ($weightedInput^{(L)}$ is the weighted input of $L$th layer and $f$ is the activation function)<br>\n",
    "&emsp;&emsp; $\\nabla_{b^{(L)}}Loss \\longleftarrow \\sum_i^{batch} {g_i}$ <br>\n",
    "&emsp;&emsp; $\\nabla_{w^{(L)}}Loss \\longleftarrow output^{(L-1)T}g$ &emsp; ($output^{(L-1)}$ is the output of $(L-1)$th layer ) <br>\n",
    "&emsp;&emsp; $g \\longleftarrow gw^{(L)T}$\n",
    "\n",
    "Check [this](http://neuralnetworksanddeeplearning.com/chap2.html) for a detailed explanation of the back-propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-bV6UF0mJMC"
   },
   "source": [
    "Now implement the back-propagation algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-pJJYYHgyKe"
   },
   "outputs": [],
   "source": [
    "def mlp_gradients(mlp, loss_function, x, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of loss with respect to mlp's weights and biases\n",
    "    Args:\n",
    "    mlp: An object of MLP class\n",
    "    loss_function: A function used as loss function of the MLP\n",
    "    x: A numpy array of shape (batch_size, input_dim). The MLP's input\n",
    "    y: A numpy array of shape (batch_size, num_classes). Target labels\n",
    "\n",
    "    Returns:\n",
    "    gradients: A list of dictionaries {'w': dw, 'b': db} corresponding to the dictionaries in mlp.parameters\n",
    "        dw is the gradient of loss with respect to the weights of the layer \n",
    "        db is the gradient of loss with respect to the bias of the layer \n",
    "\n",
    "    \"\"\"  \n",
    "\n",
    "    gradients = []\n",
    "    ### get the output of the network\n",
    "    y_hat = mlp.activations[-1]\n",
    "    num_layers = len(mlp.parameters)\n",
    "\n",
    "    ### compute gradient of the loss with respect to network output\n",
    "    g = loss_function(y, y_hat, return_grad=True)\n",
    "\n",
    "    ### You'll need the input in the last step of backprop so let's make a new list with x in the beginning\n",
    "    activations = [x] + mlp.activations \n",
    "\n",
    "    for i in reversed(range(num_layers)):\n",
    "        g = np.multiply(g, mlp.act_funcs[i](mlp.weighted_ins[i], True))\n",
    "        dw = np.dot((activations[i]).T ,g)\n",
    "        db = np.sum(g, axis=0, keepdims=True)\n",
    "        g = np.dot(g, mlp.parameters[i]['w'].T)\n",
    "        gradients = [{'w' : dw, 'b' : db}] + gradients\n",
    "\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UPjKQSS6mi_Q"
   },
   "source": [
    "### 1.5 Optimizaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe0IJelEmmXw"
   },
   "source": [
    "Now that we've computed the gradients of the parameters of our MLP, we should optimize these parameters using the gradients in order for the network to produce better outputs. <br> \n",
    "Gradient descent is an optimizaion method that iteratively moves the paramters in the oposite direction of their gradients. Below is the update rule for gradient descent:\n",
    "<br><br>\n",
    "$$ w \\leftarrow w - \\alpha \\nabla_wLoss$$ \n",
    "<br>\n",
    "Where $\\alpha$ is the learning rate hyperparameter.<br>\n",
    "There are three main variants of gradient descent: stochastic gradient descent, mini-batch gradient descent and batch gradient descent. <br>\n",
    "Mini-batch gradient descent is the most used variant in practice and that's what we'll use in this assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOx9iK3G4bef"
   },
   "source": [
    "Let's perform a step of gradient descent on a simple MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxleIKcO4ama"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss before gradient descent:  3.0806572943937454\n",
      "loss after gradient descent:  2.2295020736025606\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(size=(16, 10))\n",
    "y = np.eye(16)\n",
    "lr = 0.1\n",
    "### Define the mlp \n",
    "mlp = MLP(x.shape[-1])\n",
    "mlp.add_layer(16)\n",
    "mlp.add_layer(8)\n",
    "mlp.add_layer(y.shape[-1])\n",
    "### compute mlp's output\n",
    "y_hat = mlp.feed_forward(x)\n",
    "### print current loss\n",
    "print(\"loss before gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))\n",
    "### Compute gradients of the mlp's parameters \n",
    "grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, x, y)\n",
    "### perform gradient descent\n",
    "mlp.parameters = [{'w':p['w']-lr*g['w'], 'b':p['b']-lr*g['b']} for g, p in zip(grads, mlp.parameters)]\n",
    "### compute mlp's output again after gradeint descent\n",
    "y_hat = mlp.feed_forward(x)\n",
    "### print loss after gradient descent\n",
    "print(\"loss after gradient descent: \", softmax_categorical_cross_entropy(y, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFsBdy9v7iFs"
   },
   "source": [
    "__Question__: Do gradient descent steps always decrease the loss? why?   (Hint: toy with the learning rate in the axample above!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hYYgkfw58fm5"
   },
   "source": [
    "<font color=red>The Gradian descent with the appropriate learning rate will guild us to convergence, but for a large amount of learning rate, this will act as the contrary. But still, it doesn't mean that with an appropriate learning rate, we are always moving into better situations. It means that we will not get into worse positions. So, we whether staying at the same level or go lower. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z7mtWfd88kbA"
   },
   "source": [
    "Instead of using gradient descent, we'll be using an extention of it called gradient descent with momentum. So instead of updating the parameters based only on current gradients, we take into account the gradients from previous steps! This way, parameter updates will have lower variance and convergence will be faster and smoother. \n",
    "$$ v \\leftarrow \\gamma  v - \\alpha \\nabla_wLoss$$ \n",
    "$$ w \\leftarrow w + v$$\n",
    "Where $w$ denotes mlp's weights and $v$ is called velocity which is basically a weighted average of all previous gradients.<br>\n",
    "Here $\\gamma$ determines how fast effects of the previous gradients fade and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3s9BFOPoMDoL"
   },
   "source": [
    "Now let's implement the SGD class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O8EW1X-FNmfA"
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "\n",
    "  def __init__(self, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "  Args:\n",
    "    lr: learning rate of the SGD optimizer\n",
    "    momentum: momentum of the SGD optimizer\n",
    "\n",
    "    Hint: velocity should be a list of dictionaries just like mlp.parameters\n",
    "               \n",
    "  \"\"\" \n",
    "\n",
    "    self.lr = lr\n",
    "    self.momentum = momentum\n",
    "    ### initialize velocity\n",
    "    self.velocity = []\n",
    "  \n",
    "  def step(self, parameters, grads):\n",
    "\n",
    "    \"\"\"\n",
    "    Perform a gradient descent step\n",
    "  Args:\n",
    "    parameters: A list of dictionaries {'w': weights , 'b': bias}. MLP's parameters. \n",
    "    grads: A list of dictionaries {'w': dw, 'b': db}. gradient of MLP's parameters. Basically the output of \"mlp_gradients\" function you implemented!\n",
    "    \n",
    "  Returns:\n",
    "    Updated_parameters: A list of dictionaries {'w': weights , 'b': bias}. mlp's parameters after performing a step of gradient descent. \n",
    "               \n",
    "  \"\"\" \n",
    "    for i in range(len(parameters)):\n",
    "        self.velocity.append({'w': 0, 'b': 0})\n",
    "    \n",
    "    self.velocity = [{'w': self.momentum * v['w'] - self.lr * g['w'], 'b': self.momentum * v['b'] - self.lr * g['b']} for g, v in zip(grads,self.velocity)]\n",
    "    Updated_parameters = [{'w': p['w'] + v['w'], 'b': p['b'] + v['b']} for p, v in zip(mlp.parameters, self.velocity)]\n",
    "    \n",
    "    \n",
    "\n",
    "    return Updated_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BDqMMm-pNlVF"
   },
   "source": [
    "## 2. Classifying Kannada Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpTgQUZ7Ofcw"
   },
   "source": [
    "In this part of the assignment, you'll use the MLP you implemented in the first part to classify Kannada handwritten digits!<br> This dataset consists of 60000 images of handwritten digits in Kannada script.<br>\n",
    "You can check [this](https://github.com/vinayprabhu/Kannada_MNIST) github repository for more information about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Please Unzip the kannada file first__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9vIApVkPgMl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "train = pd.read_csv('./kannada/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oXQcevYVPpxX"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      0       0       0       0       0       0       0       0       0   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0       0       0   \n",
       "3      3       0       0       0       0       0       0       0       0   \n",
       "4      4       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GtLGkfCJQTTb"
   },
   "source": [
    "As you can see, the first column of the dataframe is the label, and the rest of the columns are the pixels. Let's put the dataset in numpy arrays. Also, we must normalize the pixel values to [0,1] range to help the convergence of our MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LDDtw13gQs--"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eb0af0d130>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = train.values[:, 1:]/255.\n",
    "y = train.values[:, 0]\n",
    "plt.imshow(x[10000].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJgwvpb5Xec4"
   },
   "source": [
    "As we are doing a multiclass classification, the labels must be in one-hot encoded format. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AB8TavCl7TSd"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(y):\n",
    "\n",
    "  y = y.reshape(-1)\n",
    "  num_samples = y.shape[0]\n",
    "  max_label = np.max(y)\n",
    "  one_hot = np.zeros((num_samples, max_label+1))\n",
    "  one_hot[np.arange(num_samples),y] = 1\n",
    "  \n",
    "  return one_hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T3c9FP4ZdZvv"
   },
   "source": [
    "Now let's transform the labels into one-hot encoded format!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DuFO6g3dWgC"
   },
   "outputs": [],
   "source": [
    "y = one_hot_encoder(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KDWfbIPzaUTB"
   },
   "source": [
    "We've implemented the _get_mini_batches_ function below. This function transforms the dataset into multiple batches. We need this function because we'll be doing mini-batch gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQwrWQdR8wCd"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_mini_batches(x, y, batch_size, shuffle=True):\n",
    "\n",
    "  idx = list(range(len(x)))\n",
    "  np.random.shuffle(idx)\n",
    "  steps = math.ceil(len(x)/batch_size)\n",
    "  x, y = x[idx, :], y[idx, :]\n",
    "  for i in range(steps):\n",
    "    yield (x[i*batch_size: (i+1)*batch_size], y[i*batch_size: (i+1)*batch_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4iQ_J2VXf2M_"
   },
   "source": [
    "Evaluation metrics are used to measure the performance of a model after training. The choice of this metric depends on factors like the nature of the task (e.g classification or regression) or a dataset's characteristics (e.g class imbalance). For multiclass classification with balanced classes, accuracy is a reasonable choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eiu967nZiU76"
   },
   "source": [
    "We've implemented the accuracy function in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JWfurLbvjDL"
   },
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "\n",
    "  return np.mean(np.argmax(y, axis=-1)==np.argmax(y_hat, axis=-1))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_RcB5fPVibUG"
   },
   "source": [
    "Now let's split the dataset into train and validatoin sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWUJ-g8bT_JC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7utzA1JimT0"
   },
   "source": [
    "Everything is now ready for training our MLP! Create your MLP model in the cell bellow. The choice of the number of layers, their sizes and their activation functions is up to you.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nHhoNHa3vz4V"
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLP(x_train.shape[-1])\n",
    "\n",
    "mlp.add_layer(6 , relu)\n",
    "mlp.add_layer(y_train.shape[-1] , relu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0-8HUbYkXCQ"
   },
   "source": [
    "Let's set some hyper-parameters. Feel free to change these hyper-parameters however you see fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3YZeR-pkBI-"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "Batch_size = 200\n",
    "sgd_lr = 0.1\n",
    "sgd_momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6RF5pttlG8l"
   },
   "source": [
    "Now let's train the network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WOUEaeTxjTIE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-71-d013e739f53d>:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for xx, yy in tqdm_notebook(mini_batches, desc='epoch {}'.format(i+1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3e1b8aedae49f38dfb67de91dae0d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 1', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 94.24 %\n",
      "test acc: 94.27 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb23b6f93dd4459b8b52f29e77993e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 2', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 95.45 %\n",
      "test acc: 95.19 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e64b20be0aa4d78a0fa9e12e0fd6b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 3', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.10 %\n",
      "test acc: 95.51 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5895c03bed4a3ba2c1ba16b6f678ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 4', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.26 %\n",
      "test acc: 95.70 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96efd2aa8704385a88c9a47c6809f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 5', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.16 %\n",
      "test acc: 95.49 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8f0e2591624d94b0fded03e7676d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 6', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.57 %\n",
      "test acc: 95.72 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42e922bf9cd547939887195d1590a32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 7', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.65 %\n",
      "test acc: 95.87 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1477709f00804bffaf7835101706c139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 8', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.92 %\n",
      "test acc: 96.15 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eba1c55f2ce424187087091e8cc6f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 9', max=1.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 96.93 %\n",
      "test acc: 96.01 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97a096430b745c2b4effce255c859a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='epoch 10', max=1.0, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training acc: 97.01 %\n",
      "test acc: 96.05 %\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "### Defining a optimizer\n",
    "optimizer = SGD(lr=sgd_lr, momentum=sgd_momentum)\n",
    "\n",
    "train_loss, val_loss, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "for i in range(epochs):\n",
    "  mini_batches = get_mini_batches(x_train, y_train, Batch_size)\n",
    "  for xx, yy in tqdm_notebook(mini_batches, desc='epoch {}'.format(i+1)):\n",
    "\n",
    "    ### forward propagation\n",
    "    mlp.feed_forward(xx)\n",
    "    ### compute gradients\n",
    "    grads = mlp_gradients(mlp, softmax_categorical_cross_entropy, xx, yy)\n",
    "    ### optimization\n",
    "    mlp.parameters = optimizer.step(mlp.parameters, grads)\n",
    "    \n",
    "  y_hat = mlp.feed_forward(x_train)\n",
    "  y_hat_val = mlp.feed_forward(x_val)\n",
    "  val_loss.append(softmax_categorical_cross_entropy(y_val, y_hat_val))\n",
    "  train_loss.append(softmax_categorical_cross_entropy(y_train, y_hat))\n",
    "  train_acc = accuracy(y_train, y_hat)*100\n",
    "  val_acc = accuracy(y_val, y_hat_val)*100\n",
    "  train_accs.append(train_acc)\n",
    "  val_accs.append(val_acc)\n",
    "  print(\"training acc: {:.2f} %\".format(train_acc))\n",
    "  print(\"test acc: {:.2f} %\".format(val_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzrzvDPXnLlI"
   },
   "source": [
    "Let's visualize accuracy and loss for train and validation sets during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R_rB9k9d2f81"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(len(train_loss))), train_loss, label='train')\n",
    "plt.plot(list(range(len(val_loss))), val_loss, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cP3Rgp99o-f7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(list(range(len(train_accs))), train_accs, label='train')\n",
    "plt.plot(list(range(len(val_accs))), val_accs, label='val')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nu0QS_stnsAX"
   },
   "source": [
    "__Question__: Looking at loss and accuracy plots, how would you describe your model in terms of bias and variance?\n",
    "\n",
    "For bias and variance you can check <a href=\"https://medium.com/@itbodhi/bias-and-variance-trade-off-542b57ac7ff4\"> This link</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFA-4VdmoL4E"
   },
   "source": [
    "<font color=red>At the start, accuracy rises and, loss reduces with the same growth but, at point 2.5, the accuracy goes down and, the loss gets higher. After point 4 accuracy rises on both training data and validation data but, they do not have the same growth anymore, in other words, we are overfitting on our training data, means that the loss on training data is lower than the loss on validation data and if we continue the difference will be high.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP using tensorflow and keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you should use keras and tensorflow to implement the exact network that you built in previous section. use the exact paramerters and then classify kannada and report the accuracy. at the end compare the resualt of 2 model you biult and explain it. \n",
    "\n",
    "If you need more information about keras and implementation you can check <a href=\"https://www.machinecurve.com/index.php/2019/07/27/how-to-create-a-basic-mlp-classifier-with-the-keras-sequential-api/\"> This link</a>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Configuration options\n",
    "feature_vector_length = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load the data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the data - MLPs do not understand such things as '2D'.\n",
    "# Reshape to 28 x 28 pixels = 784 features\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
    "\n",
    "# Convert into greyscale\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert target classes to categorical ones\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes)\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(350, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=10, batch_size=250, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Test the model after training\n",
    "test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "assignment_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
